# Standalone

- [Spark Standalone Mode](https://spark.apache.org/docs/latest/spark-standalone.html)

## Local without Hadoop

### Download

Index of [/apache/spark](http://mirror.apache-kr.org/apache/spark)

```bash
curl -O http://mirror.apache-kr.org/apache/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz
```

### Run

```bash
./bin/run-example SparkPi 10
```

### Spark Shell

```bash
./bin/spark-shell --master "local[*]"
```

---

## with Hadoop

- [Using Spark's "Hadoop Free" Build](https://spark.apache.org/docs/latest/hadoop-provided.html)

### Download

```bash
curl -O https://www.apache.org/dyn/closer.lua/spark/spark-2.4.7/spark-2.4.7-bin-without-hadoop-scala-2.12.tgz
```

### PATH

Set `SPARK_HOME`

```bash
echo 'export SPARK_HOME=/home/$USER/spark' >> ~/.bash_profile
echo 'export PATH=$PATH:$SPARK_HOME/bin' >> ~/.bash_profile
echo 'export PATH=$PATH:$SPARK_HOME/sbin' >> ~/.bash_profile
source ~/.bash_profile
```

### Configuration

```bash
mv $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh
vi $SPARK_HOME/conf/spark-env.sh
```

Insert: `export SPARK_DIST_CLASSPATH=$(hadoop classpath)`

```bash
# If 'hadoop' binary is on your PATH
export SPARK_DIST_CLASSPATH=$(hadoop classpath)

# With explicit path to 'hadoop' binary
export SPARK_DIST_CLASSPATH=$(/path/to/hadoop/bin/hadoop classpath)

# Passing a Hadoop configuration directory
export SPARK_DIST_CLASSPATH=$(hadoop --config /path/to/configs classpath)
```

---

## Examples

```bash
run-example SparkPi 10
spark-submit $SPARK_HOME/examples/src/main/python/pi.py 10

Pi is roughly 3.1421711421711422
```

---

## Cluster

```bash
echo 'export SPARK_MASTER_HOST=172.17.7.11' >> $SPARK_HOME/conf/spark-env.sh;
start-master.sh
```

```bash
start-slave.sh 172.17.7.11:7077
```
