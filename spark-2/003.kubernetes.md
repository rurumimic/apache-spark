# Kubernetes

[Running Spark on Kubernetes](https://spark.apache.org/docs/2.4.7/running-on-kubernetes.html)

## Prerequisites

- Spark 2.3+
- Kubernetes 1.6+
  - 3 CPU & 4GB RAM

## How it works

![](https://spark.apache.org/docs/latest/img/k8s-cluster-mode.png)

- Spark creates a Spark driver running within a Kubernetes pod.
- The driver creates executors which are also running within Kubernetes pods and connects to them, and executes application code.
- When the application completes, the executor pods terminate and are cleaned up, but the driver pod persists logs and remains in “completed” state in the Kubernetes API until it’s eventually garbage collected or manually cleaned up.


## Submitting Applications to Kubernetes

### Download

```bash
curl -O http://mirror.apache-kr.org/apache/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz
tar zxvf spark-2.4.7-bin-hadoop2.7.tgz
```

### Docker Images

Location: `kubernetes/dockerfiles/spark`

Run:

```bash
bin/docker-image-tool.sh -r <repo> -t my-tag build
bin/docker-image-tool.sh -r <repo> -t my-tag push
```

Example:

```bash
bin/docker-image-tool.sh build
```

```bash
docker images | grep spark

REPOSITORY    TAG       IMAGE ID        CREATED               SIZE
spark-r       latest    c69edbc544af    12 seconds ago        1.11GB
spark-py      latest    7df17ad75afe    About a minute ago    1.06GB
spark         latest    1fcc7075a892    3 minutes ago         555MB
```

### Cluster Mode

Discover the apiserver URL:

```bash
kubectl cluster-info

Kubernetes master is running at https://kubernetes.docker.internal:6443
```

#### Proxy

```bash
kubectl proxy

Starting to serve on 127.0.0.1:8001
```

### Client Mode

#### Client Mode Networking

Specify the driver’s hostname via `spark.driver.host` and your spark driver’s port to `spark.driver.port`.

#### Client Mode Executor Pod Garbage Collection

[more...](https://spark.apache.org/docs/2.4.7/running-on-kubernetes.html#client-mode-executor-pod-garbage-collection)

#### Authentication Parameters

Use the exact prefix `spark.kubernetes.authenticate` for Kubernetes authentication parameters in client mode.

### Dependency Management

[more...](https://spark.apache.org/docs/2.4.7/running-on-kubernetes.html#dependency-management)

### Secret Management

[more...](https://spark.apache.org/docs/2.4.7/running-on-kubernetes.html#secret-management)

### Using Kubernetes Volumes

[more...](https://spark.apache.org/docs/2.4.7/running-on-kubernetes.html#using-kubernetes-volumes)

### Introspection and Debugging

[more...](https://spark.apache.org/docs/2.4.7/running-on-kubernetes.html#introspection-and-debugging)

#### K8S Dashboard

K8S [Dashboard](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/)

[http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/.](http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy)

```bash
kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')
```

#### Accessing Logs

```bash
kubectl -n=<namespace> logs -f <driver-pod-name>
```

#### Accessing Driver UI

```bash
kubectl port-forward <driver-pod-name> 4040:4040
```

[http://localhost:4040](http://localhost:4040)

#### Debugging

```bash
kubectl describe pod <spark-driver-pod>
kubectl logs <spark-driver-pod>
```

### Kubernetes Features

#### RBAC

```bash
kubectl create serviceaccount spark
kubectl create clusterrolebinding spark-role --clusterrole=edit --serviceaccount=default:spark --namespace=default
```

### Run

#### Cluster

```bash
bin/spark-submit \
--master k8s://http://localhost:8001 \
--deploy-mode cluster \
--name spark-pi \
--class org.apache.spark.examples.SparkPi \
--conf spark.executor.instances=1 \
--conf spark.kubernetes.container.image=spark:latest \
--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
/opt/spark/examples/jars/spark-examples_2.11-2.4.7.jar
```

##### Result

```log
        Container name: spark-kubernetes-driver
         Container image: spark:latest
         Container state: Terminated
         Exit code: 0
20/11/12 18:07:45 INFO Client: Application spark-pi finished.
```

#### Client

```bash
bin/spark-submit \
--master k8s://http://localhost:8001 \
--deploy-mode client \
--name spark-pi \
--class org.apache.spark.examples.SparkPi \
--conf spark.executor.instances=1 \
--conf spark.kubernetes.container.image=spark:latest \
--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
$(pwd)/examples/jars/spark-examples_2.11-2.4.7.jar
```

```log
20/11/12 18:08:29 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 1.441738 s
Pi is roughly 3.1388156940784704
```
