# Standalone

- [Spark Standalone Mode](https://spark.apache.org/docs/latest/spark-standalone.html)
- [Using Spark's "Hadoop Free" Build](https://spark.apache.org/docs/latest/hadoop-provided.html)

## Install

### Download

```bash
curl -O http://mirror.apache-kr.org/spark/spark-2.4.5/spark-2.4.5-bin-without-hadoop-scala-2.12.tgz
```

### PATH

Set `SPARK_HOME`

```bash
echo 'export SPARK_HOME=/home/$USER/spark' >> ~/.bash_profile
echo 'export PATH=$PATH:$SPARK_HOME/bin' >> ~/.bash_profile
echo 'export PATH=$PATH:$SPARK_HOME/sbin' >> ~/.bash_profile
source ~/.bash_profile
```

### Configuration

```bash
mv $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh
vi $SPARK_HOME/conf/spark-env.sh
```

Insert: `export SPARK_DIST_CLASSPATH=$(hadoop classpath)`

```bash
# If 'hadoop' binary is on your PATH
export SPARK_DIST_CLASSPATH=$(hadoop classpath)

# With explicit path to 'hadoop' binary
export SPARK_DIST_CLASSPATH=$(/path/to/hadoop/bin/hadoop classpath)

# Passing a Hadoop configuration directory
export SPARK_DIST_CLASSPATH=$(hadoop --config /path/to/configs classpath)
```

---

## Examples

```bash
run-example SparkPi 10
spark-submit $SPARK_HOME/examples/src/main/python/pi.py 10

Pi is roughly 3.1421711421711422
```

---

## Cluster

```bash
echo 'export SPARK_MASTER_HOST=172.17.7.11' >> $SPARK_HOME/conf/spark-env.sh;
start-master.sh
```

```bash
start-slave.sh 172.17.7.11:7077
```
